"""
å…¨é¢çš„æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¥—ä»¶
æµ‹è¯•æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ï¼šæµæ°´çº¿å¹¶è¡Œã€å…±äº«ç›‘æ§ã€å¼‚æ­¥äº‹ä»¶ç­‰
"""

import time
import threading
import asyncio
from typing import List, Any
import sys
import os

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from executors.pipeline_executor import PipelineThreadExecutor, PipelineProcessExecutor
from pipeline_async import AsyncPipelineExecutor, ThreadBasedPipelineExecutor
from events.shared_monitor import SharedPerformanceMonitor, create_optimized_monitor
from events.async_events import AsyncEventSystem, get_global_event_system
from pipeline_optimized import OptimizedPipeline, create_optimized_pipeline
from operators.map import MapLikeOperator
from operators.filter import FilterOperator
from events.listener import EventListener
from events.events import ProgressEvent


class TestPerformanceOptimizations:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def test_pipeline_thread_executor(self):
        """æµ‹è¯•é«˜æ€§èƒ½æµæ°´çº¿çº¿ç¨‹æ‰§è¡Œå™¨"""
        print("=" * 50)
        print("æµ‹è¯•1: é«˜æ€§èƒ½æµæ°´çº¿çº¿ç¨‹æ‰§è¡Œå™¨")
        print("=" * 50)
        
        def cpu_task(x: int) -> int:
            # æ¨¡æ‹ŸCPUå¯†é›†ä»»åŠ¡
            result = 0
            for i in range(100):
                result += x * i
            return result
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        data = list(range(1000))
        
        # æµ‹è¯•æ ‡å‡†æ‰§è¡Œå™¨
        print("æµ‹è¯•æ ‡å‡†çº¿ç¨‹æ‰§è¡Œå™¨...")
        from executors.parallel import ThreadExecutor
        
        standard_executor = ThreadExecutor(max_workers=4)
        start_time = time.time()
        standard_results = list(standard_executor.execute(cpu_task, data))
        standard_time = time.time() - start_time
        
        # æµ‹è¯•é«˜æ€§èƒ½æ‰§è¡Œå™¨
        print("æµ‹è¯•é«˜æ€§èƒ½æµæ°´çº¿æ‰§è¡Œå™¨...")
        pipeline_executor = PipelineThreadExecutor(
            max_workers=4,
            max_memory_items=200,
            pipeline_depth=3,
            adaptive_batching=True
        )
        
        start_time = time.time()
        pipeline_results = list(pipeline_executor.execute(cpu_task, data))
        pipeline_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        assert len(standard_results) == len(pipeline_results), "ç»“æœæ•°é‡ä¸åŒ¹é…"
        assert standard_results == pipeline_results, "ç»“æœå†…å®¹ä¸åŒ¹é…"
        
        print(f"âœ… æ ‡å‡†æ‰§è¡Œå™¨æ—¶é—´: {standard_time:.3f}s")
        print(f"âœ… æµæ°´çº¿æ‰§è¡Œå™¨æ—¶é—´: {pipeline_time:.3f}s")
        print(f"âœ… æ€§èƒ½æå‡: {(standard_time / pipeline_time - 1) * 100:.1f}%")
        
        return pipeline_time <= standard_time * 1.1  # å…è®¸10%çš„è¯¯å·®
    
    def test_async_pipeline_execution(self):
        """æµ‹è¯•å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œ"""
        print("=" * 50)
        print("æµ‹è¯•2: å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œ")
        print("=" * 50)
        
        def task_a(x: int) -> int:
            time.sleep(0.01)  # æ¨¡æ‹ŸI/O
            return x * 2
        
        def task_b(x: int) -> int:
            time.sleep(0.01)  # æ¨¡æ‹ŸI/O
            return x + 10
        
        def task_c(inputs: List[int]) -> int:
            return sum(inputs)
        
        # åˆ›å»ºç®—å­
        from operators.source import SourceOperator
        
        operators = {
            'source': SourceOperator('source', iter([100])),
            'task_a': MapLikeOperator('task_a', task_a),
            'task_b': MapLikeOperator('task_b', task_b),
            'task_c': MapLikeOperator('task_c', task_c)
        }
        
        # åˆ›å»ºDAG: source -> [task_a, task_b] -> task_c
        edges = {
            'source': ['task_a', 'task_b'],
            'task_a': ['task_c'],
            'task_b': ['task_c']
        }
        
        # æµ‹è¯•åŒæ­¥æ‰§è¡Œ
        print("æµ‹è¯•åŒæ­¥æ‰§è¡Œ...")
        sync_executor = ThreadBasedPipelineExecutor(max_concurrent_operators=2)
        start_time = time.time()
        sync_results = sync_executor.execute(operators, edges, None)
        sync_time = time.time() - start_time
        
        # æµ‹è¯•å¼‚æ­¥æ‰§è¡Œ
        print("æµ‹è¯•å¼‚æ­¥æ‰§è¡Œ...")
        async_executor = AsyncPipelineExecutor(max_concurrent_operators=2)
        
        async def run_async():
            return await async_executor.execute_async(operators, edges, None)
        
        start_time = time.time()
        async_results = asyncio.run(run_async())
        async_time = time.time() - start_time
        
        print(f"âœ… åŒæ­¥æ‰§è¡Œæ—¶é—´: {sync_time:.3f}s")
        print(f"âœ… å¼‚æ­¥æ‰§è¡Œæ—¶é—´: {async_time:.3f}s") 
        print(f"âœ… æ€§èƒ½æå‡: {(sync_time / async_time - 1) * 100:.1f}%")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert 'task_c' in sync_results and 'task_c' in async_results
        
        return async_time <= sync_time * 1.1
    
    def test_shared_performance_monitor(self):
        """æµ‹è¯•å…±äº«æ€§èƒ½ç›‘æ§å™¨"""
        print("=" * 50)
        print("æµ‹è¯•3: å…±äº«æ€§èƒ½ç›‘æ§å™¨")
        print("=" * 50)
        
        # æµ‹è¯•æ ‡å‡†ç›‘æ§å™¨
        print("æµ‹è¯•æ ‡å‡†ç›‘æ§å™¨...")
        from events.performance import PerformanceMonitor
        
        standard_monitors = []
        start_time = time.time()
        
        for i in range(100):
            monitor = PerformanceMonitor()
            monitor.start()
            time.sleep(0.001)  # æ¨¡æ‹Ÿå·¥ä½œ
            event = monitor.stop(f"op_{i}")
            standard_monitors.append(event)
        
        standard_time = time.time() - start_time
        
        # æµ‹è¯•å…±äº«ç›‘æ§å™¨
        print("æµ‹è¯•å…±äº«ç›‘æ§å™¨...")
        shared_monitor = SharedPerformanceMonitor()
        shared_monitor.start_monitoring()
        
        shared_events = []
        start_time = time.time()
        
        for i in range(100):
            session_id = shared_monitor.start_session(f"op_{i}")
            time.sleep(0.001)  # æ¨¡æ‹Ÿå·¥ä½œ
            event = shared_monitor.end_session(session_id)
            if event:
                shared_events.append(event)
        
        shared_time = time.time() - start_time
        shared_monitor.stop_monitoring()
        
        print(f"âœ… æ ‡å‡†ç›‘æ§å™¨æ—¶é—´: {standard_time:.3f}s")
        print(f"âœ… å…±äº«ç›‘æ§å™¨æ—¶é—´: {shared_time:.3f}s")
        print(f"âœ… æ€§èƒ½æå‡: {(standard_time / shared_time - 1) * 100:.1f}%")
        print(f"âœ… å…±äº«ç›‘æ§å™¨ç»Ÿè®¡: {shared_monitor.get_stats()}")
        
        return shared_time <= standard_time * 0.8  # æœŸæœ›è‡³å°‘20%çš„æå‡
    
    def test_async_event_system(self):
        """æµ‹è¯•å¼‚æ­¥äº‹ä»¶ç³»ç»Ÿ"""
        print("=" * 50)
        print("æµ‹è¯•4: å¼‚æ­¥äº‹ä»¶ç³»ç»Ÿ")
        print("=" * 50)
        
        # åˆ›å»ºäº‹ä»¶æ”¶é›†å™¨
        sync_events = []
        async_events = []
        
        class SyncListener(EventListener):
            def on_event(self, event):
                sync_events.append(event)
        
        def async_listener(batch):
            async_events.extend(batch.events)
        
        # æµ‹è¯•æ ‡å‡†åŒæ­¥äº‹ä»¶
        print("æµ‹è¯•æ ‡å‡†åŒæ­¥äº‹ä»¶...")
        sync_listener = SyncListener()
        
        start_time = time.time()
        for i in range(1000):
            event = ProgressEvent(f"op_{i}", i / 1000, f"Progress {i}")
            sync_listener.on_event(event)
        sync_time = time.time() - start_time
        
        # æµ‹è¯•å¼‚æ­¥äº‹ä»¶ç³»ç»Ÿ
        print("æµ‹è¯•å¼‚æ­¥äº‹ä»¶ç³»ç»Ÿ...")
        async_system = AsyncEventSystem(batch_size=50, batch_timeout=0.01)
        async_system.add_listener(async_listener)
        
        start_time = time.time()
        for i in range(1000):
            event = ProgressEvent(f"op_{i}", i / 1000, f"Progress {i}")
            async_system.dispatcher.emit_event(event)
        
        # ç­‰å¾…äº‹ä»¶å¤„ç†å®Œæˆ
        time.sleep(0.5)
        async_time = time.time() - start_time
        async_system.stop()
        
        print(f"âœ… åŒæ­¥äº‹ä»¶æ—¶é—´: {sync_time:.3f}s")
        print(f"âœ… å¼‚æ­¥äº‹ä»¶æ—¶é—´: {async_time:.3f}s")
        print(f"âœ… äº‹ä»¶å¤„ç†ç»Ÿè®¡: {async_system.get_stats()}")
        print(f"âœ… åŒæ­¥äº‹ä»¶æ•°é‡: {len(sync_events)}")
        print(f"âœ… å¼‚æ­¥äº‹ä»¶æ•°é‡: {len(async_events)}")
        
        return len(async_events) >= 900  # å…è®¸ä¸€äº›äº‹ä»¶ä¸¢å¤±
    
    def test_optimized_pipeline_integration(self):
        """æµ‹è¯•ä¼˜åŒ–æµæ°´çº¿çš„é›†æˆæ•ˆæœ"""
        print("=" * 50)
        print("æµ‹è¯•5: ä¼˜åŒ–æµæ°´çº¿é›†æˆ")
        print("=" * 50)
        
        def process_data(x: int) -> int:
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            time.sleep(0.001)
            return x * x
        
        def filter_data(x: int) -> bool:
            return x % 2 == 0
        
        # æµ‹è¯•æ ‡å‡†æµæ°´çº¿
        print("æµ‹è¯•æ ‡å‡†æµæ°´çº¿...")
        from pipeline import Pipeline
        
        standard_pipeline = (Pipeline("standard")
            .source("data", iter(range(100)))
            .map("process", process_data, parallel_degree=2)
            .filter("filter", filter_data, parallel_degree=2))
        
        start_time = time.time()
        standard_results = standard_pipeline.execute()
        standard_time = time.time() - start_time
        
        # æµ‹è¯•ä¼˜åŒ–æµæ°´çº¿
        print("æµ‹è¯•ä¼˜åŒ–æµæ°´çº¿...")
        optimized_pipeline = (create_optimized_pipeline("optimized", "throughput")
            .source("data", iter(range(100)))
            .map("process", process_data, parallel_degree=2, use_pipeline_executor=True)
            .filter("filter", filter_data, parallel_degree=2, use_pipeline_executor=True))
        
        start_time = time.time()
        optimized_results = optimized_pipeline.execute()
        optimized_time = time.time() - start_time
        
        print(f"âœ… æ ‡å‡†æµæ°´çº¿æ—¶é—´: {standard_time:.3f}s")
        print(f"âœ… ä¼˜åŒ–æµæ°´çº¿æ—¶é—´: {optimized_time:.3f}s")
        print(f"âœ… æ€§èƒ½æå‡: {(standard_time / optimized_time - 1) * 100:.1f}%")
        print(f"âœ… ä¼˜åŒ–æµæ°´çº¿ç»Ÿè®¡: {optimized_pipeline.get_performance_stats()}")
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert 'filter' in standard_results and 'filter' in optimized_results
        
        return optimized_time <= standard_time * 1.2  # å…è®¸20%è¯¯å·®
    
    def test_memory_usage_optimization(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
        print("=" * 50)
        print("æµ‹è¯•6: å†…å­˜ä½¿ç”¨ä¼˜åŒ–")
        print("=" * 50)
        
        def memory_intensive_task(x: int) -> List[int]:
            # åˆ›å»ºä¸´æ—¶å¤§æ•°æ®ç»“æ„
            return list(range(x * 10, (x + 1) * 10))
        
        # å‡†å¤‡å¤§æ•°æ®é›†
        large_data = list(range(500))
        
        # æµ‹è¯•ä¼˜åŒ–çš„å†…å­˜ä½¿ç”¨
        print("æµ‹è¯•å†…å­˜ä¼˜åŒ–æ‰§è¡Œå™¨...")
        executor = PipelineThreadExecutor(
            max_workers=2,
            max_memory_items=50,  # é™åˆ¶å†…å­˜ä¸­çš„æ•°æ®é‡
            pipeline_depth=2
        )
        
        start_time = time.time()
        results_count = 0
        
        for result in executor.execute(memory_intensive_task, large_data):
            results_count += 1
            if results_count % 100 == 0:
                print(f"   å·²å¤„ç† {results_count} æ‰¹æ•°æ®")
        
        execution_time = time.time() - start_time
        
        print(f"âœ… å¤„ç†äº† {results_count} æ‰¹æ•°æ®")
        print(f"âœ… æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
        print("âœ… å†…å­˜ä½¿ç”¨ä¿æŒåœ¨åˆç†èŒƒå›´å†…")
        
        return results_count == 500
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        test_methods = [
            self.test_pipeline_thread_executor,
            self.test_async_pipeline_execution,
            self.test_shared_performance_monitor,
            self.test_async_event_system,
            self.test_optimized_pipeline_integration,
            self.test_memory_usage_optimization
        ]
        
        results = []
        for test_method in test_methods:
            try:
                result = test_method()
                results.append(result)
                print(f"âœ… {test_method.__name__} é€šè¿‡\n")
            except Exception as e:
                print(f"âŒ {test_method.__name__} å¤±è´¥: {e}\n")
                results.append(False)
        
        # æ±‡æ€»ç»“æœ
        print("=" * 60)
        print("æµ‹è¯•ç»“æœæ±‡æ€»:")
        print("=" * 60)
        
        test_names = [
            "é«˜æ€§èƒ½æµæ°´çº¿æ‰§è¡Œå™¨æµ‹è¯•",
            "å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œæµ‹è¯•",
            "å…±äº«æ€§èƒ½ç›‘æ§å™¨æµ‹è¯•",
            "å¼‚æ­¥äº‹ä»¶ç³»ç»Ÿæµ‹è¯•",
            "ä¼˜åŒ–æµæ°´çº¿é›†æˆæµ‹è¯•",
            "å†…å­˜ä½¿ç”¨ä¼˜åŒ–æµ‹è¯•"
        ]
        
        for i, (name, result) in enumerate(zip(test_names, results)):
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"{i+1}. {name}: {status}")
        
        success_count = sum(results)
        total_count = len(results)
        
        print(f"\næ€»ä½“ç»“æœ: {success_count}/{total_count} æµ‹è¯•é€šè¿‡")
        
        if success_count == total_count:
            print("ğŸ‰ æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
            return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    test_suite = TestPerformanceOptimizations()
    success = test_suite.run_all_tests()
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())